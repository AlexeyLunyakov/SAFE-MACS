{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b69ffd9",
   "metadata": {},
   "source": [
    "RT-DETR-v2 Train on custom dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6ed0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q git+https://github.com/huggingface/transformers.git\n",
    "%pip install -q git+https://github.com/roboflow/supervision.git\n",
    "%pip install -q accelerate\n",
    "%pip install -q roboflow\n",
    "%pip install -q torchmetrics\n",
    "%pip install -q \"albumentations>=1.4.5\"\n",
    "%pip install -q torchmetrics[detection]\n",
    "%pip install -q tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "056c7ae5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOME: /mnt/d/Work/coding/vuzz/new/MIREA_7_semester_VT/rsppr/SAFE-MACS/model/rt-detr-v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "HOME = os.getcwd()\n",
    "print(\"HOME:\", HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f1dd982",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import albumentations as A\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from roboflow import Roboflow\n",
    "from dataclasses import dataclass, replace\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForObjectDetection,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441c4081",
   "metadata": {},
   "source": [
    "Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21d01adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT = \"PekingU/rtdetr_r50vd_coco_o365\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(CHECKPOINT).to(DEVICE)\n",
    "processor = AutoImageProcessor.from_pretrained(CHECKPOINT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f522d53a",
   "metadata": {},
   "source": [
    "Fine-Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc231b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "ROBOFLOW_API_KEY = ('pEbpvVmHCmE4sFlvI8Og')\n",
    "rf = Roboflow(api_key=ROBOFLOW_API_KEY)\n",
    "\n",
    "project = rf.workspace(\"safemacsws\").project(\"mppe-custom-set\")\n",
    "version = project.version(4)\n",
    "dataset = version.download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "734a41b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 3480\n",
      "Number of validation images: 288\n",
      "Number of test images: 287\n"
     ]
    }
   ],
   "source": [
    "ds_train = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset.location}/train\",\n",
    "    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n",
    ")\n",
    "ds_valid = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset.location}/valid\",\n",
    "    annotations_path=f\"{dataset.location}/valid/_annotations.coco.json\",\n",
    ")\n",
    "ds_test = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset.location}/test\",\n",
    "    annotations_path=f\"{dataset.location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "print(f\"Number of training images: {len(ds_train)}\")\n",
    "print(f\"Number of validation images: {len(ds_valid)}\")\n",
    "print(f\"Number of test images: {len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b335e491",
   "metadata": {},
   "source": [
    "Data Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4723d030",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 640\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    do_resize=True,\n",
    "    size={\"width\": IMAGE_SIZE, \"height\": IMAGE_SIZE},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5735cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchDetectionDataset(Dataset):\n",
    "    def __init__(self, dataset: sv.DetectionDataset, processor, transform: A.Compose = None):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    @staticmethod\n",
    "    def annotations_as_coco(image_id, categories, boxes):\n",
    "        annotations = []\n",
    "        for category, bbox in zip(categories, boxes):\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            formatted_annotation = {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category,\n",
    "                \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
    "                \"iscrowd\": 0,\n",
    "                \"area\": (x2 - x1) * (y2 - y1),\n",
    "            }\n",
    "            annotations.append(formatted_annotation)\n",
    "\n",
    "        return {\n",
    "            \"image_id\": image_id,\n",
    "            \"annotations\": annotations,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, image, annotations = self.dataset[idx]\n",
    "\n",
    "        image = image[:, :, ::-1]\n",
    "        boxes = annotations.xyxy\n",
    "        categories = annotations.class_id\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                bboxes=boxes,\n",
    "                category=categories\n",
    "            )\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            categories = transformed[\"category\"]\n",
    "\n",
    "\n",
    "        formatted_annotations = self.annotations_as_coco(\n",
    "            image_id=idx, categories=categories, boxes=boxes)\n",
    "        result = self.processor(\n",
    "            images=image, annotations=formatted_annotations, return_tensors=\"pt\")\n",
    "\n",
    "        result = {k: v[0] for k, v in result.items()}\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a00cd6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pytorch_dataset_train = PyTorchDetectionDataset(ds_train, processor)\n",
    "pytorch_dataset_valid = PyTorchDetectionDataset(ds_valid, processor)\n",
    "pytorch_dataset_test = PyTorchDetectionDataset(ds_test, processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "73fd818f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[0.8706, 0.9176, 0.8314,  ..., 0.6784, 0.6706, 0.6588],\n",
       "          [0.8706, 0.9137, 0.8706,  ..., 0.7176, 0.7059, 0.6902],\n",
       "          [0.8667, 0.9098, 0.8784,  ..., 0.6588, 0.6588, 0.6627],\n",
       "          ...,\n",
       "          [0.7608, 0.8706, 0.7804,  ..., 0.5255, 0.5333, 0.5412],\n",
       "          [0.8784, 0.8667, 0.7725,  ..., 0.5216, 0.5137, 0.5333],\n",
       "          [0.8000, 0.8039, 0.8314,  ..., 0.5176, 0.4980, 0.5216]],\n",
       " \n",
       "         [[0.8863, 0.9333, 0.8471,  ..., 0.6784, 0.6706, 0.6588],\n",
       "          [0.8863, 0.9294, 0.8863,  ..., 0.7176, 0.7059, 0.6902],\n",
       "          [0.8824, 0.9255, 0.8941,  ..., 0.6588, 0.6588, 0.6627],\n",
       "          ...,\n",
       "          [0.7647, 0.8745, 0.7843,  ..., 0.5451, 0.5529, 0.5608],\n",
       "          [0.8824, 0.8706, 0.7765,  ..., 0.5412, 0.5333, 0.5529],\n",
       "          [0.8039, 0.8078, 0.8353,  ..., 0.5373, 0.5176, 0.5412]],\n",
       " \n",
       "         [[0.9216, 0.9686, 0.8824,  ..., 0.6863, 0.6784, 0.6667],\n",
       "          [0.9216, 0.9647, 0.9216,  ..., 0.7255, 0.7137, 0.6980],\n",
       "          [0.9176, 0.9608, 0.9294,  ..., 0.6667, 0.6667, 0.6706],\n",
       "          ...,\n",
       "          [0.7725, 0.8824, 0.7922,  ..., 0.6235, 0.6314, 0.6392],\n",
       "          [0.8902, 0.8784, 0.7843,  ..., 0.6196, 0.6118, 0.6314],\n",
       "          [0.8118, 0.8157, 0.8431,  ..., 0.6157, 0.5961, 0.6196]]]),\n",
       " 'labels': {'size': tensor([640, 640]), 'image_id': tensor([15]), 'class_labels': tensor([4, 5, 3, 1]), 'boxes': tensor([[0.6586, 0.3328, 0.2578, 0.2000],\n",
       "         [0.6582, 0.5977, 0.2070, 0.4141],\n",
       "         [0.8527, 0.4047, 0.2305, 0.3969],\n",
       "         [0.6664, 0.4977, 0.6578, 0.9953]]), 'area': tensor([ 21120.0000,  35112.5000,  37465.0000, 268177.0000]), 'iscrowd': tensor([0, 0, 0, 0]), 'orig_size': tensor([640, 640])}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_dataset_train[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3513fa5c",
   "metadata": {},
   "source": [
    "Custom data collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2f03cba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f501c07",
   "metadata": {},
   "source": [
    "mAP Compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d5e179be",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {id: label for id, label in enumerate(ds_train.classes)}\n",
    "label2id = {label: id for id, label in enumerate(ds_train.classes)}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "class MAPEvaluator:\n",
    "\n",
    "    def __init__(self, image_processor, threshold=0.00, id2label=None):\n",
    "        self.image_processor = image_processor\n",
    "        self.threshold = threshold\n",
    "        self.id2label = id2label\n",
    "\n",
    "    def collect_image_sizes(self, targets):\n",
    "        \"\"\"Collect image sizes across the dataset as list of tensors with shape [batch_size, 2].\"\"\"\n",
    "        image_sizes = []\n",
    "        for batch in targets:\n",
    "            batch_image_sizes = torch.tensor(np.array([x[\"size\"] for x in batch]))\n",
    "            image_sizes.append(batch_image_sizes)\n",
    "        return image_sizes\n",
    "\n",
    "    def collect_targets(self, targets, image_sizes):\n",
    "        post_processed_targets = []\n",
    "        for target_batch, image_size_batch in zip(targets, image_sizes):\n",
    "            for target, (height, width) in zip(target_batch, image_size_batch):\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes = sv.xcycwh_to_xyxy(boxes)\n",
    "                boxes = boxes * np.array([width, height, width, height])\n",
    "                boxes = torch.tensor(boxes)\n",
    "                labels = torch.tensor(target[\"class_labels\"])\n",
    "                post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "        return post_processed_targets\n",
    "\n",
    "    def collect_predictions(self, predictions, image_sizes):\n",
    "        post_processed_predictions = []\n",
    "        for batch, target_sizes in zip(predictions, image_sizes):\n",
    "            batch_logits, batch_boxes = batch[1], batch[2]\n",
    "            output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "            post_processed_output = self.image_processor.post_process_object_detection(\n",
    "                output, threshold=self.threshold, target_sizes=target_sizes\n",
    "            )\n",
    "            post_processed_predictions.extend(post_processed_output)\n",
    "        return post_processed_predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, evaluation_results):\n",
    "\n",
    "        predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "\n",
    "        image_sizes = self.collect_image_sizes(targets)\n",
    "        post_processed_targets = self.collect_targets(targets, image_sizes)\n",
    "        post_processed_predictions = self.collect_predictions(predictions, image_sizes)\n",
    "\n",
    "        evaluator = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "        evaluator.warn_on_many_detections = False\n",
    "        evaluator.update(post_processed_predictions, post_processed_targets)\n",
    "\n",
    "        metrics = evaluator.compute()\n",
    "\n",
    "        classes = metrics.pop(\"classes\")\n",
    "        map_per_class = metrics.pop(\"map_per_class\")\n",
    "        mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "        for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "            class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "            metrics[f\"map_{class_name}\"] = class_map\n",
    "            metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "        metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "        return metrics\n",
    "\n",
    "eval_compute_metrics_fn = MAPEvaluator(image_processor=processor, threshold=0.01, id2label=id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c861b70",
   "metadata": {},
   "source": [
    "Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "55521cf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RTDetrForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_r50vd_coco_o365 and are newly initialized because the shapes did not match:\n",
      "- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n",
      "- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    CHECKPOINT,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    anchor_image_size=None,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c2098ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num = int(input())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b6fae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./rtdetr-v2-L-mppe-{num}\",\n",
    "    num_train_epochs=2,\n",
    "    max_grad_norm=0.1,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=300,\n",
    "    per_device_train_batch_size=8,\n",
    "    dataloader_num_workers=2,\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "28ef0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=pytorch_dataset_train,\n",
    "    eval_dataset=pytorch_dataset_valid,\n",
    "    processing_class=processor,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=eval_compute_metrics_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff405740",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

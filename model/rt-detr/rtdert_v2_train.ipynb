{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train RT-DETR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import requests\n",
    "\n",
    "import numpy as np\n",
    "import supervision as sv\n",
    "import albumentations as A\n",
    "\n",
    "from PIL import Image\n",
    "from pprint import pprint\n",
    "from roboflow import Roboflow\n",
    "from dataclasses import dataclass, replace\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    AutoImageProcessor,\n",
    "    AutoModelForObjectDetection,\n",
    "    TrainingArguments,\n",
    "    Trainer\n",
    ")\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "model_check = \"PekingU/rtdetr_v2_r50vd\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(model_check).to(device)\n",
    "processor = AutoImageProcessor.from_pretrained(model_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    }
   ],
   "source": [
    "rf = Roboflow(api_key='pEbpvVmHCmE4sFlvI8Og')\n",
    "\n",
    "project = rf.workspace(\"safemacsws\").project(\"mppe-custom-set\")\n",
    "version = project.version(3)\n",
    "dataset = version.download(\"coco\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 3480\n",
      "Number of validation images: 288\n",
      "Number of test images: 287\n"
     ]
    }
   ],
   "source": [
    "ds_train = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset.location}/train\",\n",
    "    annotations_path=f\"{dataset.location}/train/_annotations.coco.json\",\n",
    ")\n",
    "ds_valid = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset.location}/valid\",\n",
    "    annotations_path=f\"{dataset.location}/valid/_annotations.coco.json\",\n",
    ")\n",
    "ds_test = sv.DetectionDataset.from_coco(\n",
    "    images_directory_path=f\"{dataset.location}/test\",\n",
    "    annotations_path=f\"{dataset.location}/test/_annotations.coco.json\",\n",
    ")\n",
    "\n",
    "print(f\"Number of training images: {len(ds_train)}\")\n",
    "print(f\"Number of validation images: {len(ds_valid)}\")\n",
    "print(f\"Number of test images: {len(ds_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 480\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\n",
    "    model_check,\n",
    "    do_resize=True,\n",
    "    size={\"width\": IMAGE_SIZE, \"height\": IMAGE_SIZE},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose(\n",
    "    [A.NoOp()],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\",\n",
    "        label_fields=[\"category\"],\n",
    "        clip=True,\n",
    "        min_area=25\n",
    "    ),\n",
    ")\n",
    "\n",
    "valid_transform = A.Compose(\n",
    "    [A.NoOp()],\n",
    "    bbox_params=A.BboxParams(\n",
    "        format=\"pascal_voc\",\n",
    "        label_fields=[\"category\"],\n",
    "        clip=True,\n",
    "        min_area=1\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchDetectionDataset(Dataset):\n",
    "    def __init__(self, dataset: sv.DetectionDataset, processor, transform: A.Compose = None):\n",
    "        self.dataset = dataset\n",
    "        self.processor = processor\n",
    "        self.transform = transform\n",
    "\n",
    "    @staticmethod\n",
    "    def annotations_as_coco(image_id, categories, boxes):\n",
    "        annotations = []\n",
    "        for category, bbox in zip(categories, boxes):\n",
    "            x1, y1, x2, y2 = bbox\n",
    "            formatted_annotation = {\n",
    "                \"image_id\": image_id,\n",
    "                \"category_id\": category,\n",
    "                \"bbox\": [x1, y1, x2 - x1, y2 - y1],\n",
    "                \"iscrowd\": 0,\n",
    "                \"area\": (x2 - x1) * (y2 - y1),\n",
    "            }\n",
    "            annotations.append(formatted_annotation)\n",
    "\n",
    "        return {\n",
    "            \"image_id\": image_id,\n",
    "            \"annotations\": annotations,\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        _, image, annotations = self.dataset[idx]\n",
    "\n",
    "        # Convert image to RGB numpy array\n",
    "        image = image[:, :, ::-1]\n",
    "        boxes = annotations.xyxy\n",
    "        categories = annotations.class_id\n",
    "\n",
    "        if self.transform:\n",
    "            transformed = self.transform(\n",
    "                image=image,\n",
    "                bboxes=boxes,\n",
    "                category=categories\n",
    "            )\n",
    "            image = transformed[\"image\"]\n",
    "            boxes = transformed[\"bboxes\"]\n",
    "            categories = transformed[\"category\"]\n",
    "\n",
    "\n",
    "        formatted_annotations = self.annotations_as_coco(\n",
    "            image_id=idx, categories=categories, boxes=boxes)\n",
    "        result = self.processor(\n",
    "            images=image, annotations=formatted_annotations, return_tensors=\"pt\")\n",
    "\n",
    "        # Image processor expands batch dimension, lets squeeze it\n",
    "        result = {k: v[0] for k, v in result.items()}\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9922, 0.9922, 0.9922],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9922, 0.9922, 0.9922],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9922, 0.9922, 0.9922]],\n",
       " \n",
       "         [[1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          ...,\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9961, 0.9961],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9961, 0.9961],\n",
       "          [1.0000, 1.0000, 1.0000,  ..., 0.9961, 0.9961, 0.9961]]]),\n",
       " 'labels': {'size': tensor([480, 480]), 'image_id': tensor([15]), 'class_labels': tensor([5, 1]), 'boxes': tensor([[0.5197, 0.2048, 0.1842, 0.0773],\n",
       "         [0.5115, 0.4257, 0.6809, 0.7872]]), 'area': tensor([  3279.0549, 123494.8672]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([343, 152])}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytorch_dataset_train = PyTorchDetectionDataset(\n",
    "    ds_train, processor, transform=train_transform)\n",
    "pytorch_dataset_valid = PyTorchDetectionDataset(\n",
    "    ds_valid, processor, transform=valid_transform)\n",
    "pytorch_dataset_test = PyTorchDetectionDataset(\n",
    "    ds_test, processor, transform=valid_transform)\n",
    "\n",
    "pytorch_dataset_train[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mAP compute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {id: label for id, label in enumerate(ds_train.classes)}\n",
    "label2id = {label: id for id, label in enumerate(ds_train.classes)}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "\n",
    "class MAPEvaluator:\n",
    "\n",
    "    def __init__(self, image_processor, threshold=0.00, id2label=None):\n",
    "        self.image_processor = image_processor\n",
    "        self.threshold = threshold\n",
    "        self.id2label = id2label\n",
    "\n",
    "    def collect_image_sizes(self, targets):\n",
    "        \"\"\"Collect image sizes across the dataset as list of tensors with shape [batch_size, 2].\"\"\"\n",
    "        image_sizes = []\n",
    "        for batch in targets:\n",
    "            batch_image_sizes = torch.tensor(np.array([x[\"size\"] for x in batch]))\n",
    "            image_sizes.append(batch_image_sizes)\n",
    "        return image_sizes\n",
    "\n",
    "    def collect_targets(self, targets, image_sizes):\n",
    "        post_processed_targets = []\n",
    "        for target_batch, image_size_batch in zip(targets, image_sizes):\n",
    "            for target, (height, width) in zip(target_batch, image_size_batch):\n",
    "                boxes = target[\"boxes\"]\n",
    "                boxes = sv.xcycwh_to_xyxy(boxes)\n",
    "                boxes = boxes * np.array([width, height, width, height])\n",
    "                boxes = torch.tensor(boxes)\n",
    "                labels = torch.tensor(target[\"class_labels\"])\n",
    "                post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "        return post_processed_targets\n",
    "\n",
    "    def collect_predictions(self, predictions, image_sizes):\n",
    "        post_processed_predictions = []\n",
    "        for batch, target_sizes in zip(predictions, image_sizes):\n",
    "            batch_logits, batch_boxes = batch[1], batch[2]\n",
    "            output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "            post_processed_output = self.image_processor.post_process_object_detection(\n",
    "                output, threshold=self.threshold, target_sizes=target_sizes\n",
    "            )\n",
    "            post_processed_predictions.extend(post_processed_output)\n",
    "        return post_processed_predictions\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def __call__(self, evaluation_results):\n",
    "\n",
    "        predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "\n",
    "        image_sizes = self.collect_image_sizes(targets)\n",
    "        post_processed_targets = self.collect_targets(targets, image_sizes)\n",
    "        post_processed_predictions = self.collect_predictions(predictions, image_sizes)\n",
    "\n",
    "        evaluator = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "        evaluator.warn_on_many_detections = False\n",
    "        evaluator.update(post_processed_predictions, post_processed_targets)\n",
    "\n",
    "        metrics = evaluator.compute()\n",
    "\n",
    "        # Replace list of per class metrics with separate metric for each class\n",
    "        classes = metrics.pop(\"classes\")\n",
    "        map_per_class = metrics.pop(\"map_per_class\")\n",
    "        mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "        for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "            class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "            metrics[f\"map_{class_name}\"] = class_map\n",
    "            metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "        metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "        return metrics\n",
    "\n",
    "eval_compute_metrics_fn = MAPEvaluator(image_processor=processor, threshold=0.01, id2label=id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RTDetrV2ForObjectDetection were not initialized from the model checkpoint at PekingU/rtdetr_v2_r50vd and are newly initialized because the shapes did not match:\n",
      "- model.decoder.class_embed.0.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.0.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.1.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.1.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.2.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.2.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.3.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.3.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.4.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.4.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.decoder.class_embed.5.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.decoder.class_embed.5.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "- model.denoising_class_embed.weight: found shape torch.Size([81, 256]) in the checkpoint and torch.Size([7, 256]) in the model instantiated\n",
      "- model.enc_score_head.bias: found shape torch.Size([80]) in the checkpoint and torch.Size([6]) in the model instantiated\n",
      "- model.enc_score_head.weight: found shape torch.Size([80, 256]) in the checkpoint and torch.Size([6, 256]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    model_check,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    anchor_image_size=None,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./rtdetr-v2-r50-mppe-1\",\n",
    "    num_train_epochs=40,\n",
    "    max_grad_norm=0.1,\n",
    "    learning_rate=5e-5,\n",
    "    warmup_steps=300,\n",
    "    per_device_train_batch_size=8,\n",
    "    dataloader_num_workers=2,\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False,\n",
    "    report_to=\"tensorboard\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=pytorch_dataset_train,\n",
    "    eval_dataset=pytorch_dataset_valid,\n",
    "    tokenizer=processor,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=eval_compute_metrics_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = []\n",
    "predictions = []\n",
    "\n",
    "for i in range(len(ds_test)):\n",
    "    path, sourece_image, annotations = ds_test[i]\n",
    "\n",
    "    image = Image.open(path)\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    w, h = image.size\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs, target_sizes=[(h, w)], threshold=0.3)\n",
    "\n",
    "    detections = sv.Detections.from_transformers(results[0])\n",
    "\n",
    "    targets.append(annotations)\n",
    "    predictions.append(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_average_precision = sv.MeanAveragePrecision.from_detections(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    ")\n",
    "\n",
    "print(f\"map50_95: {mean_average_precision.map50_95:.2f}\")\n",
    "print(f\"map50: {mean_average_precision.map50:.2f}\")\n",
    "print(f\"map75: {mean_average_precision.map75:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sv.ConfusionMatrix.from_detections(\n",
    "    predictions=predictions,\n",
    "    targets=targets,\n",
    "    classes=ds_test.classes\n",
    ")\n",
    "\n",
    "_ = confusion_matrix.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"D:/Work/coding/vuzz/new/MIREA_7_semester_VT/rsppr/SAFE-MACS/runs/detr/v2-l-mppe-1/model\")\n",
    "processor.save_pretrained(\"D:/Work/coding/vuzz/new/MIREA_7_semester_VT/rsppr/SAFE-MACS/runs/detr/v2-l-mppe-1/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRID_SIZE = 5\n",
    "\n",
    "def annotate(image, annotations, classes):\n",
    "    labels = [\n",
    "        classes[class_id]\n",
    "        for class_id\n",
    "        in annotations.class_id\n",
    "    ]\n",
    "\n",
    "    bounding_box_annotator = sv.BoundingBoxAnnotator()\n",
    "    label_annotator = sv.LabelAnnotator(text_scale=1, text_thickness=2)\n",
    "\n",
    "    annotated_image = image.copy()\n",
    "    annotated_image = bounding_box_annotator.annotate(annotated_image, annotations)\n",
    "    annotated_image = label_annotator.annotate(annotated_image, annotations, labels=labels)\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_COUNT = 5\n",
    "\n",
    "for i in range(IMAGE_COUNT):\n",
    "    path, sourece_image, annotations = ds_test[i]\n",
    "\n",
    "    image = Image.open(path)\n",
    "    inputs = processor(image, return_tensors=\"pt\").to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    w, h = image.size\n",
    "    results = processor.post_process_object_detection(\n",
    "        outputs, target_sizes=[(h, w)], threshold=0.3)\n",
    "\n",
    "    detections = sv.Detections.from_transformers(results[0]).with_nms(threshold=0.1)\n",
    "\n",
    "    annotated_images = [\n",
    "        annotate(sourece_image, annotations, ds_train.classes),\n",
    "        annotate(sourece_image, detections, ds_train.classes)\n",
    "    ]\n",
    "    grid = sv.create_tiles(\n",
    "        annotated_images,\n",
    "        titles=['ground truth', 'prediction'],\n",
    "        titles_scale=0.5,\n",
    "        single_tile_size=(400, 400),\n",
    "        tile_padding_color=sv.Color.WHITE,\n",
    "        tile_margin_color=sv.Color.WHITE\n",
    "    )\n",
    "    sv.plot_image(grid, size=(6, 6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
